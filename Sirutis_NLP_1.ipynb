{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Загрузим набор данных с помощью библиотеки Corus**"
      ],
      "metadata": {
        "id": "f-xIF3E0zCiQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Установим библиотеку Corus"
      ],
      "metadata": {
        "id": "gd-2aJUWzXIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install corus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzgnrJD_zL3q",
        "outputId": "9b146cb4-bc8f-4d2f-c9b0-50b75ac7ea6e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: corus in /usr/local/lib/python3.9/dist-packages (0.9.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сначала скачаем dataset."
      ],
      "metadata": {
        "id": "Y5PvAB_8zhYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://ai-center.botik.ru/Airec/ai-resources/Persons-1000.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g42JeHPU1SxZ",
        "outputId": "8bbf1052-5381-4aaf-9795-6d75102446bc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-27 08:58:54--  http://ai-center.botik.ru/Airec/ai-resources/Persons-1000.zip\n",
            "Resolving ai-center.botik.ru (ai-center.botik.ru)... 95.129.138.2\n",
            "Connecting to ai-center.botik.ru (ai-center.botik.ru)|95.129.138.2|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3363777 (3.2M) [application/zip]\n",
            "Saving to: ‘Persons-1000.zip.1’\n",
            "\n",
            "Persons-1000.zip.1  100%[===================>]   3.21M  2.19MB/s    in 1.5s    \n",
            "\n",
            "2023-04-27 08:58:56 (2.19 MB/s) - ‘Persons-1000.zip.1’ saved [3363777/3363777]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В самом низу предыдущей строки прописано имя сохранённого файла. Сейчас это и будет наш путь."
      ],
      "metadata": {
        "id": "kHustUgG6zQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Используем corus для загрузки."
      ],
      "metadata": {
        "id": "pL2zKPf12yu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from corus import load_persons # Для загрузки 1000 персон. Прописано в библиотеке."
      ],
      "metadata": {
        "id": "JQAdY9qr29_Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'Persons-1000.zip'        # Указатель пути\n",
        "records = load_persons(path)   # Загрузим одну часть\n",
        "next(records)                       # Вернём эту часть"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTYtErEf31Bd",
        "outputId": "7cba244d-5fe4-4649-e384-5b3615d61c14"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PersonsMarkup(\n",
              "    text='Россия рассчитывает на конструктивное воздействие США на Грузию\\r\\n\\r\\n04/08/2008 12:08\\r\\n\\r\\nМОСКВА, 4 авг - РИА Новости. Россия рассчитывает, что США воздействуют на Тбилиси в связи с обострением ситуации в зоне грузино-осетинского конфликта. Об этом статс-секретарь - заместитель министра иностранных дел России Григорий Карасин заявил в телефонном разговоре с заместителем госсекретаря США Дэниэлом Фридом.\\r\\n\\r\\n\"С российской стороны выражена глубокая озабоченность в связи с новым витком напряженности вокруг Южной Осетии, противозаконными действиями грузинской стороны по наращиванию своих вооруженных сил в регионе, бесконтрольным строительством фортификационных сооружений\", - говорится в сообщении.\\r\\n\\r\\n\"Россия уже призвала Тбилиси к ответственной линии и рассчитывает также на конструктивное воздействие со стороны Вашингтона\", - сообщил МИД России. ',\n",
              "    spans=[PersonsSpan(\n",
              "         id=1,\n",
              "         start=308,\n",
              "         stop=324,\n",
              "         value='ГРИГОРИЙ КАРАСИН'\n",
              "     ),\n",
              "     PersonsSpan(\n",
              "         id=2,\n",
              "         start=387,\n",
              "         stop=402,\n",
              "         value='ДЭНИЭЛ ФРИД'\n",
              "     )]\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=[next(records).text for i in range(999)]\n",
        "\n",
        "dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "uF5kAhvM7_Iw",
        "outputId": "c27bcda9-822b-4dda-b57d-326dbb08c52c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Комиссар СЕ критикует ограничительную политику в отношении беженцев в европейских странах\\r\\n\\r\\n05/08/2008 10:32\\r\\n\\r\\nМОСКВА, 5 августа /Новости-Грузия/.  Проводимая в европейских странах ограничительная политика в отношении беженцев нарушает ряд международных стандартов, в частности, право на воссоединение семей, заявляет Комиссар Совета Европы по правам человека Томас Хаммарберг (Thomas Hammarberg) в размещенном на его сайте еженедельном комментарии.\\r\\n\\r\\n\"Ограничительная политика в отношении беженцев в европейских странах уменьшает возможности воссоединения разделенных семей\", - полагает он.\\r\\n\\r\\nПо сообщению РИА Новости, Хаммарберг констатирует, что в последнее время \"правительства попытались ограничить приезд близких родственников к тем беженцам, которые уже проживают в стране\".\\r\\n\\r\\nКомиссар не называет конкретных стран, одновременно отмечая, что в ряде случаев подобная линия привела \"к неоправданным человеческим страданиям, когда члены семьи, зависящие друг от друга, оказались разделенными\".\\r\\n\\r\\n\"Такая политика противоречит праву на воссоединение семей, как это предусмотрено некоторыми международными стандартами\", - замечает он.\\r\\n\\r\\nКомиссар Совета Европы призывает страны учитывать в политике, проводимой в отношении беженцев, положения о семье, принятые в рамках ООН и ЕС.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Возьмём все записи и запишем их в dataset. \n",
        "P.S. Если вы меняетеколичество записываемых значений, значит нужно заново выполнять код выше на строчку, где прописывается путь. И количество не должно превышать количество значенй в загруженном датасете."
      ],
      "metadata": {
        "id": "ZMZdYPeZ_jyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[:2]     # Посмотрим первые 3 значения"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKIdl4r8AlvT",
        "outputId": "1db14640-3c9a-4c7b-b573-510d5e1d0d1a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Комиссар СЕ критикует ограничительную политику в отношении беженцев в европейских странах\\r\\n\\r\\n05/08/2008 10:32\\r\\n\\r\\nМОСКВА, 5 августа /Новости-Грузия/.  Проводимая в европейских странах ограничительная политика в отношении беженцев нарушает ряд международных стандартов, в частности, право на воссоединение семей, заявляет Комиссар Совета Европы по правам человека Томас Хаммарберг (Thomas Hammarberg) в размещенном на его сайте еженедельном комментарии.\\r\\n\\r\\n\"Ограничительная политика в отношении беженцев в европейских странах уменьшает возможности воссоединения разделенных семей\", - полагает он.\\r\\n\\r\\nПо сообщению РИА Новости, Хаммарберг констатирует, что в последнее время \"правительства попытались ограничить приезд близких родственников к тем беженцам, которые уже проживают в стране\".\\r\\n\\r\\nКомиссар не называет конкретных стран, одновременно отмечая, что в ряде случаев подобная линия привела \"к неоправданным человеческим страданиям, когда члены семьи, зависящие друг от друга, оказались разделенными\".\\r\\n\\r\\n\"Такая политика противоречит праву на воссоединение семей, как это предусмотрено некоторыми международными стандартами\", - замечает он.\\r\\n\\r\\nКомиссар Совета Европы призывает страны учитывать в политике, проводимой в отношении беженцев, положения о семье, принятые в рамках ООН и ЕС.',\n",
              " 'Пулеметы, автоматы и снайперские винтовки изъяты в арендуемом американцами доме в Бишкеке\\r\\n\\r\\n05/08/2008 10:35\\r\\n\\r\\nБИШКЕК, 5 августа /Новости-Грузия/. Правоохранительные органы Киргизии обнаружили в доме, арендуемом гражданами США в Бишкеке, пулеметы, автоматы и снайперские винтовки, сообщает во вторник пресс-служба МВД Киргизии.\\r\\n\\r\\n\"В ходе проведения оперативно-профилактического мероприятия под кодовым названием \"Арсенал\" в новостройке Ынтымак, в доме, принадлежащем 66-летнему гражданину Киргизии и арендуемом гражданами США, обнаружены и изъяты: шесть крупнокалиберных пулеметов с оптическим прицелом и с приборами ночного видения, 26 автоматов калибра 5,56 миллиметра, два винчестера марки МОСВЕГА 12-го калибра, четыре ствола от крупнокалиберного пулемета, два подствольных гранатомета, четыре снайперские винтовки с оптическим прицелом защитного цвета, шесть пистолетов калибра 9 миллиметров марки Беретта, одна винтовка\", - говорится в сообщении МВД.\\r\\n\\r\\nПресс-служба отмечает, что на момент обыска \"в доме находились несколько сотрудников посольства США, обладающих дипломатическим иммунитетом, и 10 военнослужащих, якобы прибывших из США для проведения тренинга с сотрудниками спецподразделения одной из силовых структур республики, личности которых в настоящее время устанавливаются\".\\r\\n\\r\\nСогласно сообщению, в доме было обнаружено и значительное количество боеприпасов. \"Два ножа, 2920 штук патронов калибра 5,56 миллиметра, 10556 штук патронов калибра 9 миллиметров, два ящика патронов калибра 50 миллиметров, в каждом 350 штук, патроны калибра 12 миллиметров в количестве 478 штук, маркировочные (трассирующие) патроны (красного цвета) 1000 штук, 66 штук пустых магазинов от автоматического оружия, 57 штук пустых магазинов от пистолета Беретта\", - говорится в пресс-релизе.\\r\\n\\r\\nПресс-служба МВД сообщила, что расследование по данному факту проводит прокуратура Бишкека. Сейчас выясняется, кому именно принадлежит изъятое оружие, передает РИА Новости.\\r\\n\\r\\nОружие, изъятое у граждан США правоохранительными органами Киргизии, находилось в республике с ведома правительства Киргизии, сообщил во вторник представитель пресс-службы посольства США.\\r\\n\\r\\n\"Все оборудование находилось на территории Киргизии с ведома и разрешения киргизских властей\", - сказал собеседник агентства. Военнослужащие и оружие \"прибыли в республику по приглашению правительства с целью обеспечения антитеррористических учений для министерств\", заявило американское дипломатическое ведомство.\\r\\n\\r\\n\"Дом и оборудование находились под защитой киргизских властей\", - отмечает пресс-служба.\\r\\n\\r\\nПосольство США считает случившееся \"неприятным инцидентом\" и выражает надежду что \"США и Киргизия могли бы продолжить усилия по улучшению антитеррористических возможностей Киргизии\".\\r\\n\\r\\nПресс-служба американской военной базы расположенной в международном аэропорту \"Манас\" столицы Киргизии отказалась комментировать данный инцидент с участием американских военных.\\r\\n\\r\\n\"Всеми вопросами, связанными с данным случаем, занимается посольство США\", - сообщили РИА Новости в пресс-службе базы.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим копии dataset, что бы в каждом случае использовать свой."
      ],
      "metadata": {
        "id": "4RgOvmSVhbSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_nltk = dataset.copy()\n",
        "dataset_pymorphy2 = dataset.copy()\n",
        "dataset_mystem = dataset.copy()\n",
        "dataset_spacy = dataset.copy()\n",
        "dataset_natasha = dataset.copy()"
      ],
      "metadata": {
        "id": "Igy1uO3OcpMV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Проведём нормализацию.**"
      ],
      "metadata": {
        "id": "mqIKVj42iIfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Уберём HTML разметку, если она у нас есть. С помошью библиотеки BeautifulSoup.\n",
        "\n",
        "Интересное описание тут https://docs-python.ru/packages/paket-beautifulsoup4-python/"
      ],
      "metadata": {
        "id": "UaPjaHM8iZke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup     # Импортируем BeautifulSoup"
      ],
      "metadata": {
        "id": "_rrA2l8cpJcq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для визуализации добавим неправильный элемент HTML в начало dataset, затем обработаем парсером, посмотрим на результат и уберём неправильный код. В итоге получим обработанный парсером наш dataset."
      ],
      "metadata": {
        "id": "vSF3EDuq1nUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_dataset=[]                              # Cоздадим новый список\n",
        "#new_dataset= dataset\n",
        "dataset.insert(0,\"<a></p>\")                 # Добавим неправильный код html в начало нашего списка\n",
        "\n",
        "\n",
        "print('\\n\\n ***неправильный код html в начале наше списка dataset[0] = ',dataset[0])   # Проверим неправильный код html в начале нашего списка\n",
        "\n",
        "for data in dataset :                       # Пройдёмся по всему dataset\n",
        "  soup= BeautifulSoup(data,'html.parser')   # применим к data парсер html.parser \n",
        "  new_dataset.append(str(soup))             # сохраним в new_dataset новое значение data\n",
        "  \n",
        "print('\\n\\n ***Проверим исправленный код new_dataset[0] = ',new_dataset[0])   # Проверим неправильный код html в начале нашего списка\n",
        "\n",
        "del new_dataset[0]                          # Удалим после этого этот элемент\n",
        "print('\\n\\n ***После удаления кода HTML new_dataset[0] = ',new_dataset[0])    # Посмотрим значение после удаления.\n",
        "\n",
        "dataset = new_dataset   # Сохраняем обработанный результат обратно.\n",
        "print('\\n\\n ***После обработки dataset[0] = ',dataset[0])    # Посмотрим значение после удаления."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_KXhbJPpNSM",
        "outputId": "eae2b2b0-b1f4-4143-90e3-80310fc455c0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " ***неправильный код html в начале наше списка dataset[0] =  <a></p>\n",
            "\n",
            "\n",
            " ***Проверим исправленный код new_dataset[0] =  <a></a>\n",
            "\n",
            "\n",
            " ***После удаления кода HTML new_dataset[0] =  Комиссар СЕ критикует ограничительную политику в отношении беженцев в европейских странах\r\n",
            "\r\n",
            "05/08/2008 10:32\r\n",
            "\r\n",
            "МОСКВА, 5 августа /Новости-Грузия/.  Проводимая в европейских странах ограничительная политика в отношении беженцев нарушает ряд международных стандартов, в частности, право на воссоединение семей, заявляет Комиссар Совета Европы по правам человека Томас Хаммарберг (Thomas Hammarberg) в размещенном на его сайте еженедельном комментарии.\r\n",
            "\r\n",
            "\"Ограничительная политика в отношении беженцев в европейских странах уменьшает возможности воссоединения разделенных семей\", - полагает он.\r\n",
            "\r\n",
            "По сообщению РИА Новости, Хаммарберг констатирует, что в последнее время \"правительства попытались ограничить приезд близких родственников к тем беженцам, которые уже проживают в стране\".\r\n",
            "\r\n",
            "Комиссар не называет конкретных стран, одновременно отмечая, что в ряде случаев подобная линия привела \"к неоправданным человеческим страданиям, когда члены семьи, зависящие друг от друга, оказались разделенными\".\r\n",
            "\r\n",
            "\"Такая политика противоречит праву на воссоединение семей, как это предусмотрено некоторыми международными стандартами\", - замечает он.\r\n",
            "\r\n",
            "Комиссар Совета Европы призывает страны учитывать в политике, проводимой в отношении беженцев, положения о семье, принятые в рамках ООН и ЕС.\n",
            "\n",
            "\n",
            " ***После обработки dataset[0] =  Комиссар СЕ критикует ограничительную политику в отношении беженцев в европейских странах\r\n",
            "\r\n",
            "05/08/2008 10:32\r\n",
            "\r\n",
            "МОСКВА, 5 августа /Новости-Грузия/.  Проводимая в европейских странах ограничительная политика в отношении беженцев нарушает ряд международных стандартов, в частности, право на воссоединение семей, заявляет Комиссар Совета Европы по правам человека Томас Хаммарберг (Thomas Hammarberg) в размещенном на его сайте еженедельном комментарии.\r\n",
            "\r\n",
            "\"Ограничительная политика в отношении беженцев в европейских странах уменьшает возможности воссоединения разделенных семей\", - полагает он.\r\n",
            "\r\n",
            "По сообщению РИА Новости, Хаммарберг констатирует, что в последнее время \"правительства попытались ограничить приезд близких родственников к тем беженцам, которые уже проживают в стране\".\r\n",
            "\r\n",
            "Комиссар не называет конкретных стран, одновременно отмечая, что в ряде случаев подобная линия привела \"к неоправданным человеческим страданиям, когда члены семьи, зависящие друг от друга, оказались разделенными\".\r\n",
            "\r\n",
            "\"Такая политика противоречит праву на воссоединение семей, как это предусмотрено некоторыми международными стандартами\", - замечает он.\r\n",
            "\r\n",
            "Комиссар Совета Европы призывает страны учитывать в политике, проводимой в отношении беженцев, положения о семье, принятые в рамках ООН и ЕС.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "А теперь объеденим наш dataset  в один список, добавим в начало неправильный элемент, обработаем парсером, получим текст и посмотрим результат. В итоге получим обработанный начальный список."
      ],
      "metadata": {
        "id": "GXLeZI4a2dBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_dataset = ' '.join(dataset)    # Обьеденим в общий список наш  dataset\n",
        "\n",
        "list_dataset = \"<a></p>\" + list_dataset   # Добавим в наш список код HTML\n",
        "print('\\n\\n list_dataset[:40] = ', list_dataset[:40])    # Посмотрим результат.\n",
        "\n",
        "soup_list= BeautifulSoup(list_dataset,'html.parser')   # применим к list_dataset парсер html.parser \n",
        "list_dataset = str(soup_list.get_text())    # Занесём в наш списисок полусенный текст преобразованный в тип str.\n",
        "\n",
        "\n",
        "print('\\n\\n list_dataset[:40] = ', list_dataset[:40])   # Посмотрим результат."
      ],
      "metadata": {
        "id": "hnS8QCYkoJ8q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ff6b91c-04de-4d2e-eca1-9bec79a6583b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " list_dataset[:40] =  <a></p>Комиссар СЕ критикует ограничител\n",
            "\n",
            "\n",
            " list_dataset[:40] =  Комиссар СЕ критикует ограничительную по\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Уберём CSS разметку, если она у нас есть. С помошью библиотеки BeautifulSoup для нашего списка."
      ],
      "metadata": {
        "id": "V2QBbLNs9V_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for data in soup_list(['style', 'script']):\n",
        "  data.decompose()\n",
        "list_dataset = ' '.join(soup_list.stripped_strings)"
      ],
      "metadata": {
        "id": "xcJz6z6viIDW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Уберём Emoji если онb у нас есть. С помошью библиотеки Clean для нашего списка."
      ],
      "metadata": {
        "id": "r643AS1VCKW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install clean-text    # Установим библиотеку clean"
      ],
      "metadata": {
        "id": "L7sHlWkOiIAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d2cb196-33ea-49be-85f3-1c33fd0312c5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: clean-text in /usr/local/lib/python3.9/dist-packages (0.6.0)\n",
            "Requirement already satisfied: ftfy<7.0,>=6.0 in /usr/local/lib/python3.9/dist-packages (from clean-text) (6.1.1)\n",
            "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from clean-text) (1.7.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "metadata": {
        "id": "rmXoEKKFCkkH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b44f2331-01bf-4262-fc9a-8748bb1fcdfd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.9/dist-packages (1.3.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unidecode import unidecode"
      ],
      "metadata": {
        "id": "10MFcrvTCbHn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from cleantext import clean   # Импортируем метод clean\n",
        "\n",
        "emoji_dataset = list_dataset    # сохраним наш текст\n",
        "\n",
        "emoji_dataset=clean(emoji_dataset, no_emoji=True )"
      ],
      "metadata": {
        "id": "OljU4C_jiH9Z"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Проведём токенизацию**"
      ],
      "metadata": {
        "id": "-VFRrlhKIiwe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1. Используем **NLTK**"
      ],
      "metadata": {
        "id": "-s5P0MAw39KN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk   # Импортируем nltk\n",
        "nltk.download('punkt')    # Загрузим punkt\n",
        "\n",
        "from nltk.tokenize import (\n",
        "    sent_tokenize,    # Токенизация по предложения\n",
        "    word_tokenize,    # Токенизация по словам\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pxsz2w8w3cj9",
        "outputId": "96061dc4-5997-4678-c49f-2df993b4faf6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.1 Разбиение на предложения используя\n",
        "**Sent_tokenize**"
      ],
      "metadata": {
        "id": "4axmDj3l5ziP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Применим токенизацию на предложения, с указанием параметра что язык русский\n",
        "nltk_sent_rus_list_dataset = nltk.sent_tokenize(list_dataset, language = 'russian')   \n",
        "\n",
        "print('Всего получено предложений : ', len(nltk_sent_rus_list_dataset),'\\n')\n",
        "print('Разделение на предложения, с указанием языка : \\n')\n",
        "nltk_sent_rus_list_dataset[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ooRG8Vj_Pp4",
        "outputId": "f3bd9ddd-5f05-498f-89b7-cd38a6a9f9bf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Всего получено предложений :  12532 \n",
            "\n",
            "Разделение на предложения, с указанием языка : \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Комиссар СЕ критикует ограничительную политику в отношении беженцев в европейских странах\\r\\n\\r\\n05/08/2008 10:32\\r\\n\\r\\nМОСКВА, 5 августа /Новости-Грузия/.',\n",
              " 'Проводимая в европейских странах ограничительная политика в отношении беженцев нарушает ряд международных стандартов, в частности, право на воссоединение семей, заявляет Комиссар Совета Европы по правам человека Томас Хаммарберг (Thomas Hammarberg) в размещенном на его сайте еженедельном комментарии.',\n",
              " '\"Ограничительная политика в отношении беженцев в европейских странах уменьшает возможности воссоединения разделенных семей\", - полагает он.',\n",
              " 'По сообщению РИА Новости, Хаммарберг констатирует, что в последнее время \"правительства попытались ограничить приезд близких родственников к тем беженцам, которые уже проживают в стране\".',\n",
              " 'Комиссар не называет конкретных стран, одновременно отмечая, что в ряде случаев подобная линия привела \"к неоправданным человеческим страданиям, когда члены семьи, зависящие друг от друга, оказались разделенными\".']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2. Разбиение на предложения используя\n",
        "**Word_tokenize**"
      ],
      "metadata": {
        "id": "Z0Q6e-H6DCW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Применим токенизацию на слова, с указанием параметра что язык русский\n",
        "nltk_word_rus_list_dataset = nltk.word_tokenize(list_dataset, language = 'russian')   \n",
        "\n",
        "print('Всего получено слов : ', len(nltk_word_rus_list_dataset),'\\n')\n",
        "print('Разделение на слова, с указанием языка : \\n')\n",
        "nltk_word_rus_list_dataset[:5]"
      ],
      "metadata": {
        "id": "03bnCEBQiH2M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c38530fa-e712-4a80-bc67-4f67d4ec987a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Всего получено слов :  259594 \n",
            "\n",
            "Разделение на слова, с указанием языка : \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Комиссар', 'СЕ', 'критикует', 'ограничительную', 'политику']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3. Разделим на слова используя пробелы в качестве разделителя с помщью функции **WhitespaceTokenizer**"
      ],
      "metadata": {
        "id": "MLs_59r3Cce1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "nltk_space_rus_list_dataset = WhitespaceTokenizer().tokenize(list_dataset)   \n",
        "\n",
        "print('Всего получено слов : ', len(nltk_space_rus_list_dataset),'\\n')\n",
        "print('Разделение на слова по пробелам: \\n')\n",
        "nltk_space_rus_list_dataset[:5]\n"
      ],
      "metadata": {
        "id": "1HexP6K0iHYV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f11751d-3539-4e8f-d19a-7e771fa7b5d0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Всего получено слов :  220682 \n",
            "\n",
            "Разделение на слова по пробелам: \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Комиссар', 'СЕ', 'критикует', 'ограничительную', 'политику']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разделим на слова используя пунктуацию в качестве разделителя с помщью функции **WordPunctTokenizer**"
      ],
      "metadata": {
        "id": "LrxC4FeDEawt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "nltk_punct_rus_list_dataset = WordPunctTokenizer().tokenize(list_dataset)   \n",
        "\n",
        "print('Всего получено слов : ', len(nltk_punct_rus_list_dataset),'\\n')\n",
        "print('Разделение на слова по пунктуации: \\n')\n",
        "nltk_punct_rus_list_dataset[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgAb3EdqCYgl",
        "outputId": "7c91787a-027c-444b-893a-02425d700be1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Всего получено слов :  268993 \n",
            "\n",
            "Разделение на слова по пунктуации: \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Комиссар', 'СЕ', 'критикует', 'ограничительную', 'политику']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.4. Для примера, разделим слова на слоги, и посмотрим что получилось.\n",
        "Используем **SyllableTokenizer**."
      ],
      "metadata": {
        "id": "Pm0h09wqRmvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import SyllableTokenizer\n",
        "\n",
        "syll_tokenizer = SyllableTokenizer(\n",
        "    lang='ru',    # Язык русский\n",
        "    sonority_hierarchy=[\n",
        "        'аеёиоуыэюя',   # гласные\n",
        "        'мн',   # носовые\n",
        "        'вфзсжшх',    # фрикативные\n",
        "        'рпбдткгщ',   # стоповые\n",
        "    ]\n",
        ")\n",
        "\n",
        "nltk_syll_rus_list_dataset=[]\n",
        "\n",
        "for data in nltk_punct_rus_list_dataset:    # Прогоним по нашему датасету.\n",
        "  nltk_syll_rus_list_dataset.append(syll_tokenizer.tokenize(data))    # Добавить в новый датасет раздённые слова по слогам.\n",
        "\n",
        "\n",
        "print('Всего получено слов : ', len(nltk_syll_rus_list_dataset),'\\n')\n",
        "print('Разделение на слоги: \\n')\n",
        "nltk_syll_rus_list_dataset[:15]"
      ],
      "metadata": {
        "id": "n1-v5YaWCYdk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16b43743-bbd5-42f9-95ee-bae142280c9e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ч'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'л'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ь'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ц'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'й'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'T'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'h'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'o'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'm'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'a'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 's'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'H'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'r'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'b'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'e'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'g'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ъ'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '№'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'Ч'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'E'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'v'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'z'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'I'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'P'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'O'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'M'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'i'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 't'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'l'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'A'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'c'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'S'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'B'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'C'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'n'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'V'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'd'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'R'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'u'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'Й'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'N'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'Л'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'Ъ'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'Ц'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'k'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'D'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'p'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'G'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'y'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'j'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'F'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'W'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'J'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'Z'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'Y'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'L'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'U'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'K'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'w'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '–'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'f'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '«'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '»'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '—'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '©'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '­'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'x'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'Q'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '“'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '”'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'q'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '…'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'X'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '€'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'і'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '•'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'Ь'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Всего получено слов :  268993 \n",
            "\n",
            "Разделение на слоги: \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Ко', 'мис', 'сар'],\n",
              " ['СЕ'],\n",
              " ['кри', 'ти', 'кует'],\n",
              " ['ог', 'ра', 'нич', 'и', 'тел', 'ь', 'ную'],\n",
              " ['пол', 'и', 'ти', 'ку'],\n",
              " ['в'],\n",
              " ['о', 'тно', 'ше', 'нии'],\n",
              " ['бе', 'же', 'нцев'],\n",
              " ['в'],\n",
              " ['ев', 'ро', 'пейс', 'ких'],\n",
              " ['стра', 'нах'],\n",
              " ['05'],\n",
              " ['/'],\n",
              " ['08'],\n",
              " ['/']]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2 Используем **Spacy**"
      ],
      "metadata": {
        "id": "WH3lsItmsXqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy    # Импортируем spacy"
      ],
      "metadata": {
        "id": "EUDZBOh8CYa1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m spacy download ru_core_news_sm    # Скачаем Russian pipeline optimized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTJAcEKnrx2s",
        "outputId": "860bee87-26dd-4f5a-ff36-b6ac57622321"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-27 08:59:47.257689: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ru-core-news-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.5.0/ru_core_news_sm-3.5.0-py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.9/dist-packages (from ru-core-news-sm==3.5.0) (3.5.2)\n",
            "Requirement already satisfied: pymorphy3>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from ru-core-news-sm==3.5.0) (1.2.0)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.5.0) (0.7.2)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.9/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.5.0) (0.6.2)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.9/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.5.0) (2.4.417150.4580142)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"ru_core_news_sm\")\n",
        "nlp.max_length =10**8   #увеличим максимальную длинну \n",
        "\n",
        "nlp_list_dataset = nlp(list_dataset)    # Получим результат\n",
        "nlp_list_dataset[:10]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuJ9-CDQrxzO",
        "outputId": "3c707205-b7b7-4cd2-ab29-190e0f782a5c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Комиссар СЕ критикует ограничительную политику в отношении беженцев в европейских"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Удаление стоп слов. Используем NLTK.**"
      ],
      "metadata": {
        "id": "MXjQGsuTvIIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import nltk    # Не забываем про это. У нас уже раньше использовалось.\n",
        "nltk.download('stopwords')    # Cкачаем стоп слова\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "rus_stop_words = stopwords.words(\"russian\")    # Получим русские стоп слова.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sWzOelvt3mN",
        "outputId": "9e108bc8-3c59-4889-c612-f12d352e82ac"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nostopword_nltk_punct_rus_list_dataset=[]\n",
        "\n",
        "print('Количество токенов в датасете до удаления до удаления стоп слов : ', len(nltk_punct_rus_list_dataset))\n",
        "\n",
        "for token1 in nltk_punct_rus_list_dataset:    # Пройдемся по датасету\n",
        "    if token1 not in rus_stop_words:    # Проверим на стоп слова\n",
        "        nostopword_nltk_punct_rus_list_dataset.append(token1)\n",
        "\n",
        "print('Количество токенов в датасете после удаления стоп слов : ', len(nostopword_nltk_punct_rus_list_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnHEptaPt3aJ",
        "outputId": "ecd7ac85-470e-46b8-dda2-bfca29874ef0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество токенов в датасете до удаления до удаления стоп слов :  268993\n",
            "Количество токенов в датасете после удаления стоп слов :  224356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Лемматизация**"
      ],
      "metadata": {
        "id": "6ESd6eKAwBti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.1. Используем **pymorphy2**"
      ],
      "metadata": {
        "id": "xpSV3u0AoPYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggnZqFVjrxv6",
        "outputId": "63157e0f-1e11-48fb-ac57-4cee7d0ca90d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.9/dist-packages (0.9.1)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.9/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.9/dist-packages (from pymorphy2) (0.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pymorphy2    # Импортируем\n",
        "morph = pymorphy2.MorphAnalyzer()   # Создадим модель\n",
        "pym2_nostopword_nltk_punct_rus_list_dataset =[]   \n",
        "\n",
        "for doc in nostopword_nltk_punct_rus_list_dataset:   # Пройдём по всему датасету\n",
        "  pym2_nostopword_nltk_punct_rus_list_dataset.append(morph.parse(doc)[0].normal_form)   # Положим в новый датасет полученные результаты.\n",
        "\n",
        "pym2_nostopword_nltk_punct_rus_list_dataset[:25]    # Посмотрим 10 значений"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae9EgWnarxsK",
        "outputId": "7744137c-0698-4631-93d9-f7edcc4d1ad7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['комиссар',\n",
              " 'сие',\n",
              " 'критиковать',\n",
              " 'ограничительный',\n",
              " 'политика',\n",
              " 'отношение',\n",
              " 'беженец',\n",
              " 'европейский',\n",
              " 'страна',\n",
              " '05',\n",
              " '/',\n",
              " '08',\n",
              " '/',\n",
              " '2008',\n",
              " '10',\n",
              " ':',\n",
              " '32',\n",
              " 'москва',\n",
              " ',',\n",
              " '5',\n",
              " 'август',\n",
              " '/',\n",
              " 'новость',\n",
              " '-',\n",
              " 'грузия']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.2 Используем **Spacy**"
      ],
      "metadata": {
        "id": "MXUKuDx9ocMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spasy_lemma_nostopword_nlp_list_dataset=[]\n",
        "\n",
        "for token in nlp_list_dataset:    # Проходим по датасету, которй раньше уже был сделан nlp.\n",
        "  if token.is_stop == False :   # Если это не стопслово\n",
        "    spasy_lemma_nostopword_nlp_list_dataset.append(token.lemma_)    # Тогда запишем в новый датасет лемматизированную форму данного слова.\n",
        "    \n",
        "spasy_lemma_nostopword_nlp_list_dataset[:15]    # Посмотрим 15 значений))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ChPyLo8oYxO",
        "outputId": "eb6d7c9d-4fbd-4f1d-9c6a-201e50a243ee"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['комиссар',\n",
              " 'критиковать',\n",
              " 'ограничительный',\n",
              " 'политика',\n",
              " 'отношение',\n",
              " 'беженец',\n",
              " 'европейский',\n",
              " 'страна',\n",
              " '\\r\\n\\r\\n',\n",
              " '05/08/2008',\n",
              " '10:32',\n",
              " '\\r\\n\\r\\n',\n",
              " 'москва',\n",
              " ',',\n",
              " '5']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print ('Итоговый dataset с использованием pymorphy2 :\\n')\n",
        "print (pym2_nostopword_nltk_punct_rus_list_dataset[:10],'\\n')\n",
        "print (pym2_nostopword_nltk_punct_rus_list_dataset[10:20],'\\n\\n')\n",
        "\n",
        "print ('Итоговый dataset с использованием spacy :\\n')\n",
        "print (spasy_lemma_nostopword_nlp_list_dataset[:10],'\\n')\n",
        "print (spasy_lemma_nostopword_nlp_list_dataset[10:20],'\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnkmqoWJrxor",
        "outputId": "94240f59-e963-44fa-adcc-d79220ad1bef"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Итоговый dataset с использованием pymorphy2 :\n",
            "\n",
            "['комиссар', 'сие', 'критиковать', 'ограничительный', 'политика', 'отношение', 'беженец', 'европейский', 'страна', '05'] \n",
            "\n",
            "['/', '08', '/', '2008', '10', ':', '32', 'москва', ',', '5'] \n",
            "\n",
            "\n",
            "Итоговый dataset с использованием spacy :\n",
            "\n",
            "['комиссар', 'критиковать', 'ограничительный', 'политика', 'отношение', 'беженец', 'европейский', 'страна', '\\r\\n\\r\\n', '05/08/2008'] \n",
            "\n",
            "['10:32', '\\r\\n\\r\\n', 'москва', ',', '5', 'август', '/новости', '-', 'грузия/.', ' '] \n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}